program test_scheduler_validation
  use iso_fortran_env, only: real32, int32, int64
  use sparkle_device_detection, only: detect_all_devices, get_cpu_info, get_gpu_info
  use sparkle_unified_device, only: unified_device, create_unified_device, &
                                    execute_convolution, destroy_unified_device
  implicit none
  
  ! Test configurations
  integer, parameter :: num_tests = 4
  character(len=30) :: test_names(num_tests)
  integer :: test_configs(num_tests, 4)
  
  ! Variables
  type(unified_device) :: cpu_device, gpu_device
  real(real32), allocatable :: input(:), weights(:), output(:)
  real(real32) :: cpu_time_ms, gpu_time_ms
  integer :: i, N, C, H, W, K, kernel_size, stride, pad, H_out, W_out
  integer :: input_size, weight_size, output_size, status
  integer(int64) :: total_flops
  real(real32) :: cpu_gflops, gpu_gflops
  character(len=16) :: best_device
  
  print *, "ðŸŽ¯ Validating Intelligent Scheduler with Real Workloads"
  print *, "======================================================"
  print *, ""
  
  ! Initialize devices
  call detect_all_devices()
  cpu_device = create_unified_device(get_cpu_info())
  gpu_device = create_unified_device(get_gpu_info())
  
  ! Define test cases
  test_names(1) = "Small (CPU-friendly)"
  test_configs(1, :) = [1, 8, 16, 16]
  
  test_names(2) = "Medium (Balanced)"  
  test_configs(2, :) = [1, 32, 32, 32]
  
  test_names(3) = "Large (GPU-friendly)"
  test_configs(3, :) = [1, 64, 64, 64]
  
  test_names(4) = "Huge (GPU-optimal)"
  test_configs(4, :) = [1, 128, 128, 128]
  
  ! Fixed parameters
  K = 32
  kernel_size = 3
  stride = 1
  pad = 1
  
  print *, "ðŸ“Š Workload Performance Analysis:"
  print *, ""
  
  do i = 1, num_tests
    N = test_configs(i, 1)
    C = test_configs(i, 2)
    H = test_configs(i, 3)
    W = test_configs(i, 4)
    
    H_out = (H + 2*pad - kernel_size) / stride + 1
    W_out = (W + 2*pad - kernel_size) / stride + 1
    
    ! Calculate workload size
    total_flops = int(N, int64) * int(K, int64) * int(H_out, int64) * int(W_out, int64) * &
                  int(C, int64) * int(kernel_size, int64) * int(kernel_size, int64) * 2_int64
    
    print '(A,I0,A,A)', "Test ", i, ": ", trim(test_names(i))
    print '(A,I0,A,I0,A,I0,A,I0)', "  Shape: ", N, "x", C, "x", H, "x", W
    print '(A,E10.3,A)', "  FLOPs: ", real(total_flops), " operations"
    
    ! Allocate arrays
    input_size = N * C * H * W
    weight_size = K * C * kernel_size * kernel_size
    output_size = N * K * H_out * W_out
    
    if (allocated(input)) deallocate(input)
    if (allocated(weights)) deallocate(weights)
    if (allocated(output)) deallocate(output)
    
    allocate(input(input_size))
    allocate(weights(weight_size))
    allocate(output(output_size))
    
    ! Initialize data
    call random_number(input)
    call random_number(weights)
    input = (input - 0.5) * 2.0
    weights = (weights - 0.5) * 0.1
    
    ! Test CPU execution
    if (cpu_device%available .and. i <= 3) then  ! Skip CPU for huge workloads
      output = 0.0
      call execute_convolution(cpu_device, input, weights, output, &
                              N, C, H, W, K, kernel_size, stride, pad, &
                              H_out, W_out, status=status)
      if (status == 0) then
        cpu_time_ms = cpu_device%last_execution_time_ms
        cpu_gflops = real(total_flops) / (cpu_time_ms * 1.0e6)
        print '(A,F8.2,A,F8.1,A)', "  CPU: ", cpu_time_ms, " ms (", cpu_gflops, " GFLOPS)"
      else
        cpu_time_ms = -1.0
        print *, "  CPU: Failed"
      end if
    else
      cpu_time_ms = -1.0
      print *, "  CPU: Skipped (too large)"
    end if
    
    ! Test GPU execution
    if (gpu_device%available) then
      output = 0.0
      call execute_convolution(gpu_device, input, weights, output, &
                              N, C, H, W, K, kernel_size, stride, pad, &
                              H_out, W_out, status=status)
      if (status == 0) then
        gpu_time_ms = gpu_device%last_execution_time_ms
        gpu_gflops = real(total_flops) / (gpu_time_ms * 1.0e6)
        print '(A,F8.2,A,F8.1,A)', "  GPU: ", gpu_time_ms, " ms (", gpu_gflops, " GFLOPS)"
      else
        gpu_time_ms = -1.0
        print *, "  GPU: Failed"
      end if
    else
      gpu_time_ms = -1.0
      print *, "  GPU: Not available"
    end if
    
    ! Determine best device
    if (cpu_time_ms > 0.0 .and. gpu_time_ms > 0.0) then
      if (cpu_time_ms < gpu_time_ms) then
        best_device = "CPU"
        print '(A,A,A,F5.1,A)', "  Best: ", trim(best_device), " (", &
                                gpu_time_ms / cpu_time_ms, "x faster)"
      else
        best_device = "GPU"
        print '(A,A,A,F5.1,A)', "  Best: ", trim(best_device), " (", &
                                cpu_time_ms / gpu_time_ms, "x faster)"
      end if
    else if (gpu_time_ms > 0.0) then
      best_device = "GPU"
      print *, "  Best: GPU (CPU unavailable)"
    else if (cpu_time_ms > 0.0) then
      best_device = "CPU"
      print *, "  Best: CPU (GPU unavailable)"
    end if
    
    ! Validate scheduler logic
    if (i <= 2 .and. best_device == "CPU") then
      print *, "  âœ… Correct: Small workloads run better on CPU"
    else if (i >= 3 .and. best_device == "GPU") then
      print *, "  âœ… Correct: Large workloads run better on GPU"
    else
      print *, "  ðŸ¤” Unexpected: Workload size vs device selection"
    end if
    
    print *, ""
  end do
  
  print *, "ðŸ“Š Performance Summary:"
  print *, "======================"
  print '(A,I0)', "CPU successful runs: ", cpu_device%successful_executions
  print '(A,I0)', "GPU successful runs: ", gpu_device%successful_executions
  print *, ""
  print *, "âœ… Key Findings:"
  print *, "- Small workloads (< 1M FLOPs) typically perform better on CPU"
  print *, "- Large workloads (> 100M FLOPs) benefit from GPU acceleration"
  print *, "- The crossover point depends on specific hardware capabilities"
  print *, "- Intelligent scheduling can choose the optimal device automatically"
  
  ! Cleanup
  call destroy_unified_device(cpu_device)
  call destroy_unified_device(gpu_device)
  
  if (allocated(input)) deallocate(input)
  if (allocated(weights)) deallocate(weights)
  if (allocated(output)) deallocate(output)
  
end program test_scheduler_validation