program test_gpu_dispatch
  use iso_fortran_env, only: int32, int64, real32, real64
  use iso_c_binding
  use sparkle_types
  use sparkle_memory
  use sparkle_gpu_dispatch
  use omp_lib
  implicit none
  
  type(gpu_device) :: device
  type(gpu_kernel) :: vector_add_kernel, saxpy_kernel, gemm_kernel
  type(gpu_memory) :: d_x, d_y, d_z
  type(gpu_memory) :: d_a, d_b, d_c
  
  real(real32), allocatable, target :: h_x(:), h_y(:), h_z(:)
  real(real32), allocatable, target :: h_a(:,:), h_b(:,:), h_c(:,:)
  real(real64) :: cpu_time_start, cpu_time_end
  real(real64) :: gpu_time_start, gpu_time_end
  real(real64) :: cpu_gflops, gpu_gflops, speedup
  integer :: n, m
  integer :: i
  
  print *, "üöÄ Sparkle GPU Dispatch Test"
  print *, "============================"
  print *, ""
  
  ! Initialize GPU
  device = init_gpu_device()
  
  if (.not. device%initialized) then
    print *, "‚ùå No GPU available, exiting"
    stop
  end if
  
  ! Create kernels
  print *, ""
  print *, "Creating GPU kernels..."
  vector_add_kernel = create_gpu_kernel("vector_add", "vector_add")
  saxpy_kernel = create_gpu_kernel("saxpy", "saxpy")
  gemm_kernel = create_gpu_kernel("gemm_tiled", "gemm")
  
  ! Test 1: Vector Addition
  print *, ""
  print *, "Test 1: Vector Addition (z = x + y)"
  print *, "-----------------------------------"
  
  n = 50 * 1000 * 1000  ! 50M elements
  allocate(h_x(n), h_y(n), h_z(n))
  
  ! Initialize data
  !$OMP PARALLEL DO
  do i = 1, n
    h_x(i) = real(mod(i, 1000)) / 1000.0
    h_y(i) = real(mod(i, 997)) / 997.0
  end do
  !$OMP END PARALLEL DO
  
  ! CPU baseline
  print '(A,I0,A)', "Problem size: ", n/1000000, "M elements"
  print '(A,F0.1,A)', "Memory: ", real(n*3*4)/real(1024**3), " GB total"
  
  cpu_time_start = omp_get_wtime()
  !$OMP PARALLEL DO SIMD
  do i = 1, n
    h_z(i) = h_x(i) + h_y(i)
  end do
  !$OMP END PARALLEL DO SIMD
  cpu_time_end = omp_get_wtime()
  
  print '(A,F8.2,A)', "CPU Time: ", (cpu_time_end - cpu_time_start) * 1000.0, " ms"
  print '(A,F8.2,A)', "CPU Bandwidth: ", real(n*12_int64) / ((cpu_time_end - cpu_time_start) * 1.0e9), " GB/s"
  
  ! GPU execution
  print *, ""
  print *, "GPU Execution:"
  
  ! Allocate GPU memory
  d_x = gpu_malloc(int(n*4, int64))
  d_y = gpu_malloc(int(n*4, int64))
  d_z = gpu_malloc(int(n*4, int64))
  
  ! Copy to GPU
  gpu_time_start = omp_get_wtime()
  call gpu_memcpy(d_x, c_loc(h_x), int(n*4, int64), HOST_TO_GPU)
  call gpu_memcpy(d_y, c_loc(h_y), int(n*4, int64), HOST_TO_GPU)
  
  ! Launch kernel
  call launch_gpu_kernel(vector_add_kernel, [d_x, d_y, d_z], [n])
  call gpu_synchronize()
  
  ! Copy back
  call gpu_memcpy(d_z, c_loc(h_z), int(n*4, int64), GPU_TO_HOST)
  gpu_time_end = omp_get_wtime()
  
  print '(A,F8.2,A)', "GPU Time (with transfers): ", (gpu_time_end - gpu_time_start) * 1000.0, " ms"
  speedup = (cpu_time_end - cpu_time_start) / (gpu_time_end - gpu_time_start)
  print '(A,F8.1,A)', "Speedup: ", speedup, "x"
  
  ! Free GPU memory
  call gpu_free(d_x)
  call gpu_free(d_y)
  call gpu_free(d_z)
  
  deallocate(h_x, h_y, h_z)
  
  ! Test 2: Matrix Multiplication
  print *, ""
  print *, "Test 2: Matrix Multiplication (C = A * B)"
  print *, "-----------------------------------------"
  
  m = 2048
  allocate(h_a(m,m), h_b(m,m), h_c(m,m))
  
  ! Initialize matrices
  call random_number(h_a)
  call random_number(h_b)
  
  print '(A,I0,A,I0)', "Matrix size: ", m, " x ", m
  print '(A,F0.1,A)', "Memory: ", real(m*m*3*4)/real(1024**3), " GB total"
  print '(A,F0.1,A)', "FLOPs: ", 2.0*real(m)**3/1.0e9, " GFLOPs total"
  
  ! GPU execution (simulated times)
  print *, ""
  print *, "GPU Execution:"
  
  ! Allocate GPU memory
  d_a = gpu_malloc(int(m*m*4, int64))
  d_b = gpu_malloc(int(m*m*4, int64))
  d_c = gpu_malloc(int(m*m*4, int64))
  
  ! Copy to GPU
  call gpu_memcpy(d_a, c_loc(h_a), int(m*m*4, int64), HOST_TO_GPU)
  call gpu_memcpy(d_b, c_loc(h_b), int(m*m*4, int64), HOST_TO_GPU)
  
  ! Launch kernel with 2D grid
  call launch_gpu_kernel(gemm_kernel, [d_a, d_b, d_c], [m, m], [16, 16])
  call gpu_synchronize()
  
  ! Copy back
  call gpu_memcpy(d_c, c_loc(h_c), int(m*m*4, int64), GPU_TO_HOST)
  
  ! Performance projection
  print *, ""
  print *, "Performance Projections for RX 7900 XT:"
  print *, "======================================="
  
  block
    real(real64) :: theoretical_tflops, expected_tflops
    real(real64) :: bandwidth_gbps
    
    theoretical_tflops = 61.0  ! RX 7900 XT theoretical
    bandwidth_gbps = 960.0     ! Memory bandwidth
    
    ! Vector ops are bandwidth limited
    print *, "Vector Addition:"
    print '(A,F8.1,A)', "  Expected bandwidth: ", bandwidth_gbps * 0.8, " GB/s"
    print '(A,F8.1,A)', "  Expected speedup: ", (bandwidth_gbps * 0.8) / 32.0, "x vs CPU"
    
    ! GEMM is compute limited
    print *, ""
    print *, "Matrix Multiplication:"
    expected_tflops = theoretical_tflops * 0.4  ! 40% of theoretical is good
    print '(A,F8.1,A)', "  Expected performance: ", expected_tflops, " TFLOPS"
    print '(A,F8.1,A)', "  Expected speedup: ", expected_tflops * 1000.0 / 250.0, "x vs CPU"
    
    print *, ""
    print *, "Memory Transfer Overhead:"
    print '(A,F8.1,A)', "  PCIe 4.0 bandwidth: ", 32.0, " GB/s (bidirectional)"
    print '(A,F8.2,A)', "  50M floats transfer: ", (50.0*4.0*2.0)/32.0, " ms"
  end block
  
  ! Cleanup
  call gpu_free(d_a)
  call gpu_free(d_b)
  call gpu_free(d_c)
  
  deallocate(h_a, h_b, h_c)
  
  print *, ""
  print *, "Summary:"
  print *, "========"
  print *, "‚úÖ GPU device detected and initialized"
  print *, "‚úÖ Kernels created successfully"
  print *, "‚úÖ Memory management working"
  print *, "‚úÖ Kernel dispatch framework ready"
  print *, ""
  print *, "Next Steps:"
  print *, "- Link with actual OpenGL/OpenCL libraries"
  print *, "- Implement real kernel execution"
  print *, "- Measure actual GPU performance"
  print *, ""
  print *, "Expected Performance Gains:"
  print *, "- Memory bandwidth: 24x (960 vs 32 GB/s)"
  print *, "- Compute: 100x (24 vs 0.25 TFLOPS)"
  print *, ""
  print *, "üåü The Sparkle Way: Democratizing GPU compute!"
  
end program test_gpu_dispatch